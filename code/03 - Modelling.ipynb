{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bdc4209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f517dd43",
   "metadata": {},
   "source": [
    "## TensorFlow.data API Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "595b671f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_ShuffleDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# listing files to later read them and their label\n",
    "list_ds = tf.data.Dataset.list_files('../data/*/*.wav')\n",
    "# checking the type of list_ds variable\n",
    "list_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb992de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of the dataset\n",
    "len(list_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c012fd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorSpec(shape=(), dtype=tf.string, name=None)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# inspect the type of each element component\n",
    "print(list_ds.element_spec)\n",
    "\n",
    "# Use value_type to see the type of value represented by the element spec\n",
    "print(list_ds.element_spec.value_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "054401c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function that will receive the filename in the list_ds\n",
    "# and return the spectrogram of the file and its label\n",
    "def load_audio(filename):\n",
    "    #getting the label from the filename\n",
    "    parts = tf.strings.split(filename, sep='-')\n",
    "    label = int(parts[-5])-1\n",
    "    # loading the file using librosa\n",
    "    filename = filename.numpy().decode('utf-8')\n",
    "    audio, sr = librosa.load(filename)\n",
    "    # setting a threshold of 30 below reference to be considered as silence\n",
    "    audio, _ = librosa.effects.trim(audio, top_db=30)\n",
    "    # setting the same length\n",
    "    target_size = 88200\n",
    "    if len(audio) > target_size:\n",
    "        audio = audio[:target_size]\n",
    "    elif len(audio) < target_size:\n",
    "        audio = librosa.util.pad_center(audio, size=target_size, axis=0) # 88200 is 4s of audio, at sample rate 22050\n",
    "    # getting the melspectrogram of the audio\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels = 128, )\n",
    "    # converting the amplitude to db - approximates to how humans perceive sound\n",
    "    spectrogram = librosa.amplitude_to_db(S, ref = np.max)\n",
    "    #expanding dimension to match the shape expected in the convolutional NN    \n",
    "    spectrogram = np.expand_dims(spectrogram, axis=2)\n",
    "    # convert the spectrogram to a TensorFlow tensor to use tensorflow pipeline\n",
    "    spectrogram = tf.convert_to_tensor(spectrogram, dtype=tf.float32) \n",
    "    return spectrogram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e44d0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying load_audio in an individual file\n",
    "# spec shape will be used in modelling\n",
    "file_path = next(iter(list_ds))\n",
    "spec, label = load_audio(file_path)\n",
    "print(spec.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6f43cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(128, 173, 1), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying load_audio to the whole list_ds dataset\n",
    "# this map method using python functions was extracted from tensorflow.data API tutorials\n",
    "audios_ds = list_ds.map(lambda filename:\\\n",
    "                tf.py_function(func=load_audio, inp=[filename], Tout=[tf.float32, tf.int32]))\n",
    "audios_ds = audios_ds.map(lambda spectrogram, label: (tf.reshape(spectrogram, [128,173, 1]), tf.reshape(label,[])))\n",
    "# now audios_ds contains the spectrograms and their respective labels\n",
    "audios_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71d06ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the size of the dataset to further split some percentage of it into training and testing set\n",
    "size_dataset = len(list(audios_ds))\n",
    "size_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f57b0d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset that is currently ordered by class\n",
    "audios_ds = audios_ds.shuffle(buffer_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05c0e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking 80% of the dataset to training set and the remaining 20% to the testing set\n",
    "train = audios_ds.take(int(0.8*size_dataset)).cache().batch(32).prefetch(8)\n",
    "test = audios_ds.skip(int(0.8*size_dataset)).take(int(0.2*size_dataset)).cache().batch(32).prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7521a26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (128, 173, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 16:18:29.729968: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 637 of 1000\n",
      "2023-08-01 16:18:36.026124: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_3 (Normaliza  (None, 128, 173, 1)       3         \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 124, 169, 64)      1664      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 122, 167, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 61, 83, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 61, 83, 128)       0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 648064)            0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               82952320  \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83028875 (316.73 MB)\n",
      "Trainable params: 83028872 (316.73 MB)\n",
      "Non-trainable params: 3 (16.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = spec.shape\n",
    "print('Input shape:', input_shape)\n",
    "\n",
    "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
    "norm_layer = layers.Normalization()\n",
    "# Fit the state of the layer to the spectrograms\n",
    "# with `Normalization.adapt`.\n",
    "norm_layer.adapt(data=train.map(map_func=lambda spec, label: spec))\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    # Normalize.\n",
    "    norm_layer,\n",
    "    layers.Conv2D(64, 5, activation='relu'),\n",
    "    layers.Conv2D(128, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(8, activation = 'softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0125551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='rmsprop',\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d265633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36/36 [==============================] - ETA: 0s - loss: 6.2315 - acc: 0.2188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 16:19:31.364459: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 597 of 1000\n",
      "2023-08-01 16:19:37.742994: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 63s 2s/step - loss: 6.2315 - acc: 0.2188 - val_loss: 1.5902 - val_acc: 0.5972\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 39s 1s/step - loss: 1.5165 - acc: 0.4575 - val_loss: 1.0573 - val_acc: 0.7882\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 39s 1s/step - loss: 0.9738 - acc: 0.6649 - val_loss: 0.6858 - val_acc: 0.7743\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 39s 1s/step - loss: 0.5057 - acc: 0.8368 - val_loss: 0.3951 - val_acc: 0.8819\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.2409 - acc: 0.9184 - val_loss: 0.3674 - val_acc: 0.8993\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.1653 - acc: 0.9470 - val_loss: 0.4090 - val_acc: 0.8993\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.1126 - acc: 0.9514 - val_loss: 0.5912 - val_acc: 0.8785\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.0682 - acc: 0.9766 - val_loss: 0.4455 - val_acc: 0.8993\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.0488 - acc: 0.9870 - val_loss: 0.5013 - val_acc: 0.8993\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=4),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='../models/model_tf.h5')]\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=test,\n",
    "    epochs=10,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f8106d",
   "metadata": {},
   "source": [
    "The validation accuracy 89.9% is a huge improvement from the baseline of 13.3%. The model is looking great. <br>\n",
    "To be conversative of the performance of the model, other format of modelling will be tested to check this accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a32e4fc",
   "metadata": {},
   "source": [
    "# Modelling with numpy format as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19dedc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the metadata csv file to apply load_audio to the pandas series with the filepaths and labels\n",
    "files_details = pd.read_csv('../csvs/files_details.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "84921f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the function to apply 'load_audio' to the pandas series with the filepaths and labels\n",
    "def load_audio(filename, label):\n",
    "    # extracting the numpy array and the sampling rate of the signal\n",
    "    audio, sr = librosa.load(filename)\n",
    "    # setting a threshold of 30 below reference to be considered as silence\n",
    "    audio, _ = librosa.effects.trim(audio, top_db=30)\n",
    "    # setting the same length for every audio\n",
    "    target_size = 88200\n",
    "    if len(audio) > target_size:\n",
    "        audio = audio[:target_size]\n",
    "    elif len(audio) < target_size:\n",
    "        audio = librosa.util.pad_center(audio, size=target_size, axis=0) # 88200 is 4s of audio, at sample rate 22050  \n",
    "    # creating the melspectogram\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels = 128, )\n",
    "    # setting db instead of amplitude - approximates of how humans perceive sound\n",
    "    spectrogram = librosa.amplitude_to_db(S, ref = np.max)\n",
    "    # expanding the dimension so that it matches the format expected in the convolutional NN\n",
    "    spectrogram = np.expand_dims(spectrogram, axis=2)\n",
    "    \n",
    "    return spectrogram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b100c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using load_audio to generate the X (spectrograms) and y (labels) to input to the model\n",
    "X, y = [], []\n",
    "for file_path, label in zip(files_details['file'], files_details['label']):\n",
    "    spectrogram, label = load_audio(file_path, label)\n",
    "    X.append(spectrogram)\n",
    "    y.append(label)\n",
    "\n",
    "# transforming to arrays and reshaping y to match the format expected in the convolutional NN \n",
    "X = np.array(X)\n",
    "y = np.array(y, dtype=np.int32)\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d6adacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ef68c50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_9 (Normaliza  (None, 128, 173, 1)       3         \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 124, 169, 64)      1664      \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 122, 167, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 61, 83, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 61, 83, 128)       0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 648064)            0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               82952320  \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83028875 (316.73 MB)\n",
      "Trainable params: 83028872 (316.73 MB)\n",
      "Non-trainable params: 3 (16.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiating tf.keras.layers.Normalization layer\n",
    "norm_layer = layers.Normalization()\n",
    "# adapting the layer to the training set to add to the model\n",
    "norm_layer.adapt(data=X_train)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=X_train[0].shape),\n",
    "    norm_layer,\n",
    "    layers.Conv2D(64, 5, activation='relu'),\n",
    "    layers.Conv2D(128, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(8, activation = 'softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a59bf091",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='rmsprop',\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f7ffaace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "36/36 [==============================] - 38s 1s/step - loss: 7.8426 - acc: 0.2378 - val_loss: 1.7738 - val_acc: 0.3681\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 39s 1s/step - loss: 1.6226 - acc: 0.4141 - val_loss: 1.6898 - val_acc: 0.3750\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 38s 1s/step - loss: 1.1493 - acc: 0.6016 - val_loss: 1.3140 - val_acc: 0.5382\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.6970 - acc: 0.7630 - val_loss: 1.2711 - val_acc: 0.5451\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.4261 - acc: 0.8568 - val_loss: 1.3515 - val_acc: 0.5556\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.2625 - acc: 0.9141 - val_loss: 1.5322 - val_acc: 0.5729\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.1711 - acc: 0.9401 - val_loss: 1.4664 - val_acc: 0.5590\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 39s 1s/step - loss: 0.1360 - acc: 0.9592 - val_loss: 1.7319 - val_acc: 0.6007\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=4),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='../models/model_old_format_2.h5')]\n",
    "\n",
    "history = model.fit( x = X_train,\n",
    "    y = y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6953b",
   "metadata": {},
   "source": [
    "The validation accuracy using the numpy format is much lower. It is possible that this difference is due to potential data leakage in the other format. Other methods to split the data will be testes in the other format. The model considered will be this model that has numpy array as inputs and has an accuracy of 60%. <br>\n",
    "The validation accuracy of 60% is much better than the baseline 13.3%. This shows there are potential uses and improvement for the model. <br>\n",
    "Some suggestions may be to incorporate more datasets, files, incorporating more emotions, different actors and different phrases so that it is more generalizable. <br>\n",
    "Since the model is overfit, further regularizations through L1, L2 regularizers and testing different dropouts could possibly lead to improved overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
